{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Precision, Recall, F1, and plot confusion matrix libs\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CIFAR-10 batch file\n",
    "def load_cifar10_train(root_dir):\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    for i in range(1, 6):\n",
    "        file_name = os.path.join(root_dir, f\"data_batch_{i}\")\n",
    "        with open(file_name, 'rb') as fo:\n",
    "            data_dict = pickle.load(fo, encoding='bytes')\n",
    "            # data_dict[b'data'] shape: (10000, 3072)\n",
    "            # data_dict[b'labels'] length: 10000\n",
    "            all_data.append(data_dict[b'data'])\n",
    "            all_labels.append(data_dict[b'labels'])\n",
    "            \n",
    "    # 拼接\n",
    "    all_data = np.concatenate(all_data, axis=0)  # (50000, 3072)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)  # (50000,)\n",
    "    \n",
    "    # 把 numpy 转为 torch.Tensor\n",
    "    # 先 reshape 成 (N, 3, 32, 32)\n",
    "    all_data = all_data.reshape(-1, 3, 32, 32).astype(np.float32)\n",
    "    # 转成 tensor\n",
    "    X_train = torch.from_numpy(all_data)  # shape: (50000, 3, 32, 32)\n",
    "    y_train = torch.from_numpy(all_labels).long()  # shape: (50000,)\n",
    "    \n",
    "    return X_train, y_train\n",
    "\n",
    "def load_cifar10_test(root_dir):\n",
    "    \"\"\"\n",
    "    加载 test_batch 文件并返回 (X_test, y_test)\n",
    "    \"\"\"\n",
    "    file_name = os.path.join(root_dir, \"test_batch\")\n",
    "    with open(file_name, 'rb') as fo:\n",
    "        data_dict = pickle.load(fo, encoding='bytes')\n",
    "        test_data = data_dict[b'data']  # shape: (10000, 3072)\n",
    "        test_labels = data_dict[b'labels']  # list of length 10000\n",
    "    \n",
    "    test_data = test_data.reshape(-1, 3, 32, 32).astype(np.float32)\n",
    "    X_test = torch.from_numpy(test_data)\n",
    "    y_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "    \n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设你的数据目录是：\n",
    "cifar_root = './datasets/cifar-10-batches-py'\n",
    "\n",
    "# 加载训练与测试\n",
    "X_train, y_train = load_cifar10_train(cifar_root)  # shape: (50000, 3, 32, 32), (50000,)\n",
    "X_test, y_test   = load_cifar10_test(cifar_root)   # shape: (10000, 3, 32, 32), (10000,)\n",
    "\n",
    "# 在 MLP 中，我们通常会 flatten 图像 (3*32*32=3072)。这里可以做两种方法：\n",
    "# 方法A：在创建 dataset 时就直接 flatten\n",
    "# 方法B：在 forward 时 flatten\n",
    "# 这里演示方法A\n",
    "\n",
    "X_train = X_train.view(X_train.size(0), -1)  # (50000, 3072)\n",
    "X_test  = X_test.view(X_test.size(0), -1)    # (10000, 3072)\n",
    "\n",
    "# 用 TensorDataset 直接打包 (data, label)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset  = TensorDataset(X_test, y_test)\n",
    "\n",
    "# 用 DataLoader 来分批迭代\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim=3072, hidden_dims=[512, 256, 128], num_classes=10):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = h\n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 3072)\n",
    "        return self.model(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Train Loss: 4.1193, Train Acc: 0.2674, Test Loss: 1.9009, Test Acc: 0.3309\n",
      "Epoch [2/20] Train Loss: 1.7996, Train Acc: 0.3597, Test Loss: 1.7669, Test Acc: 0.3656\n",
      "Epoch [3/20] Train Loss: 1.7274, Train Acc: 0.3855, Test Loss: 1.7575, Test Acc: 0.3818\n",
      "Epoch [4/20] Train Loss: 1.6749, Train Acc: 0.4048, Test Loss: 1.6626, Test Acc: 0.4064\n",
      "Epoch [5/20] Train Loss: 1.6261, Train Acc: 0.4217, Test Loss: 1.6734, Test Acc: 0.4020\n",
      "Epoch [6/20] Train Loss: 1.6024, Train Acc: 0.4282, Test Loss: 1.6286, Test Acc: 0.4101\n",
      "Epoch [7/20] Train Loss: 1.5799, Train Acc: 0.4387, Test Loss: 1.5770, Test Acc: 0.4355\n",
      "Epoch [8/20] Train Loss: 1.5505, Train Acc: 0.4462, Test Loss: 1.6372, Test Acc: 0.4201\n",
      "Epoch [9/20] Train Loss: 1.5352, Train Acc: 0.4518, Test Loss: 1.5772, Test Acc: 0.4414\n",
      "Epoch [10/20] Train Loss: 1.5222, Train Acc: 0.4538, Test Loss: 1.5344, Test Acc: 0.4587\n",
      "Epoch [11/20] Train Loss: 1.5004, Train Acc: 0.4639, Test Loss: 1.5693, Test Acc: 0.4334\n",
      "Epoch [12/20] Train Loss: 1.4999, Train Acc: 0.4657, Test Loss: 1.5581, Test Acc: 0.4364\n",
      "Epoch [13/20] Train Loss: 1.4891, Train Acc: 0.4669, Test Loss: 1.5600, Test Acc: 0.4442\n",
      "Epoch [14/20] Train Loss: 1.4701, Train Acc: 0.4749, Test Loss: 1.5143, Test Acc: 0.4712\n",
      "Epoch [15/20] Train Loss: 1.4630, Train Acc: 0.4786, Test Loss: 1.5133, Test Acc: 0.4566\n",
      "Epoch [16/20] Train Loss: 1.4377, Train Acc: 0.4847, Test Loss: 1.4853, Test Acc: 0.4746\n",
      "Epoch [17/20] Train Loss: 1.4433, Train Acc: 0.4827, Test Loss: 1.4830, Test Acc: 0.4751\n",
      "Epoch [18/20] Train Loss: 1.4327, Train Acc: 0.4874, Test Loss: 1.5086, Test Acc: 0.4637\n",
      "Epoch [19/20] Train Loss: 1.4088, Train Acc: 0.4960, Test Loss: 1.4803, Test Acc: 0.4754\n",
      "Epoch [20/20] Train Loss: 1.4060, Train Acc: 0.4965, Test Loss: 1.4601, Test Acc: 0.4807\n"
     ]
    }
   ],
   "source": [
    "model = SimpleMLP(input_dim=3072, hidden_dims=[512,256,128], num_classes=10).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    test_loss, test_acc   = eval_epoch(model, test_loader, criterion)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4841, Recall: 0.4807, F1: 0.4776\n",
      "Confusion Matrix:\n",
      " [[495  34  81  24  31  26  15  35 227  32]\n",
      " [ 31 529   8  31  13  15  11  25 142 195]\n",
      " [ 55  31 273  74 189 119 124  75  48  12]\n",
      " [ 27  23  79 328  47 243 101  58  53  41]\n",
      " [ 43  13  98  54 433  91 132  82  44  10]\n",
      " [ 17  13  71 201  65 416  67  75  55  20]\n",
      " [  6  16  52 111 121 110 512  23  33  16]\n",
      " [ 30  14  53  58 110  74  31 548  35  47]\n",
      " [ 64  41  14  30  19  17   8  17 746  44]\n",
      " [ 29 136   9  45  16  17  17  50 154 527]]\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "    \n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall    = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1        = f1_score(all_labels, all_preds, average='macro')\n",
    "    cm        = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return precision, recall, f1, cm\n",
    "\n",
    "# 使用示例：在最终训练完成后\n",
    "precision, recall, f1, cm = compute_metrics(model, test_loader)\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
